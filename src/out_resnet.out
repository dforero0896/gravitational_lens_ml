No GPU found
Physical CPU found. Num Physical CPUs Available:  1
[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU')]
Logical CPU found. Num logical CPUs Available:  1
[LogicalDevice(name='/job:localhost/replica:0/task:0/device:CPU:0', device_type='CPU')]

Configuration file:

Section: general
  workdir = /home/epfl/variu/phd/gravitational_lens_ml
  train_multiband = /home/epfl/dforero/gravitational_lens_ml/data/train_multiband_noclip_bin
  use_gpu = 1
Section: trainparams
  test_fraction = 0.33
  batch_size = 10
  subsample_train = 5000
  epochs = 50
  n = 3
  resnetversion = 1
  augment_train_data = 1
  data_bias = raw
Section: bands
  vis0 = 1
  nir1 = 0
  nir2 = 0
  nir3 = 0
Project directory: /home/epfl/variu/phd/gravitational_lens_ml
The shape of the image catalog: (100009, 26)

The number of lenses is  54723
The number of lenses is  54723
The number of lenses is  15092
The number of lenses is  15092
The number of objects in the whole training sample is:  46776
The number of objects in the whole validation sample is:  23039
The test fraction is:  0.33
The number of objects in the training subsample is:  5000
The number of objects in the validation subsample is:  2462
The number of training steps is:  500
The number of validation steps is:  246
The bands are:  [True, False, False, False]
Learning rate:  0.001
Model: "model"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_1 (InputLayer)            [(None, 200, 200, 1) 0                                            
__________________________________________________________________________________________________
conv2d (Conv2D)                 (None, 200, 200, 16) 160         input_1[0][0]                    
__________________________________________________________________________________________________
batch_normalization (BatchNorma (None, 200, 200, 16) 64          conv2d[0][0]                     
__________________________________________________________________________________________________
activation (Activation)         (None, 200, 200, 16) 0           batch_normalization[0][0]        
__________________________________________________________________________________________________
conv2d_1 (Conv2D)               (None, 200, 200, 16) 2320        activation[0][0]                 
__________________________________________________________________________________________________
batch_normalization_1 (BatchNor (None, 200, 200, 16) 64          conv2d_1[0][0]                   
__________________________________________________________________________________________________
activation_1 (Activation)       (None, 200, 200, 16) 0           batch_normalization_1[0][0]      
__________________________________________________________________________________________________
conv2d_2 (Conv2D)               (None, 200, 200, 16) 2320        activation_1[0][0]               
__________________________________________________________________________________________________
batch_normalization_2 (BatchNor (None, 200, 200, 16) 64          conv2d_2[0][0]                   
__________________________________________________________________________________________________
add (Add)                       (None, 200, 200, 16) 0           activation[0][0]                 
                                                                 batch_normalization_2[0][0]      
__________________________________________________________________________________________________
activation_2 (Activation)       (None, 200, 200, 16) 0           add[0][0]                        
__________________________________________________________________________________________________
conv2d_3 (Conv2D)               (None, 200, 200, 16) 2320        activation_2[0][0]               
__________________________________________________________________________________________________
batch_normalization_3 (BatchNor (None, 200, 200, 16) 64          conv2d_3[0][0]                   
__________________________________________________________________________________________________
activation_3 (Activation)       (None, 200, 200, 16) 0           batch_normalization_3[0][0]      
__________________________________________________________________________________________________
conv2d_4 (Conv2D)               (None, 200, 200, 16) 2320        activation_3[0][0]               
__________________________________________________________________________________________________
batch_normalization_4 (BatchNor (None, 200, 200, 16) 64          conv2d_4[0][0]                   
__________________________________________________________________________________________________
add_1 (Add)                     (None, 200, 200, 16) 0           activation_2[0][0]               
                                                                 batch_normalization_4[0][0]      
__________________________________________________________________________________________________
activation_4 (Activation)       (None, 200, 200, 16) 0           add_1[0][0]                      
__________________________________________________________________________________________________
conv2d_5 (Conv2D)               (None, 200, 200, 16) 2320        activation_4[0][0]               
__________________________________________________________________________________________________
batch_normalization_5 (BatchNor (None, 200, 200, 16) 64          conv2d_5[0][0]                   
__________________________________________________________________________________________________
activation_5 (Activation)       (None, 200, 200, 16) 0           batch_normalization_5[0][0]      
__________________________________________________________________________________________________
conv2d_6 (Conv2D)               (None, 200, 200, 16) 2320        activation_5[0][0]               
__________________________________________________________________________________________________
batch_normalization_6 (BatchNor (None, 200, 200, 16) 64          conv2d_6[0][0]                   
__________________________________________________________________________________________________
add_2 (Add)                     (None, 200, 200, 16) 0           activation_4[0][0]               
                                                                 batch_normalization_6[0][0]      
__________________________________________________________________________________________________
activation_6 (Activation)       (None, 200, 200, 16) 0           add_2[0][0]                      
__________________________________________________________________________________________________
conv2d_7 (Conv2D)               (None, 100, 100, 32) 4640        activation_6[0][0]               
__________________________________________________________________________________________________
batch_normalization_7 (BatchNor (None, 100, 100, 32) 128         conv2d_7[0][0]                   
__________________________________________________________________________________________________
activation_7 (Activation)       (None, 100, 100, 32) 0           batch_normalization_7[0][0]      
__________________________________________________________________________________________________
conv2d_8 (Conv2D)               (None, 100, 100, 32) 9248        activation_7[0][0]               
__________________________________________________________________________________________________
conv2d_9 (Conv2D)               (None, 100, 100, 32) 544         activation_6[0][0]               
__________________________________________________________________________________________________
batch_normalization_8 (BatchNor (None, 100, 100, 32) 128         conv2d_8[0][0]                   
__________________________________________________________________________________________________
add_3 (Add)                     (None, 100, 100, 32) 0           conv2d_9[0][0]                   
                                                                 batch_normalization_8[0][0]      
__________________________________________________________________________________________________
activation_8 (Activation)       (None, 100, 100, 32) 0           add_3[0][0]                      
__________________________________________________________________________________________________
conv2d_10 (Conv2D)              (None, 100, 100, 32) 9248        activation_8[0][0]               
__________________________________________________________________________________________________
batch_normalization_9 (BatchNor (None, 100, 100, 32) 128         conv2d_10[0][0]                  
__________________________________________________________________________________________________
activation_9 (Activation)       (None, 100, 100, 32) 0           batch_normalization_9[0][0]      
__________________________________________________________________________________________________
conv2d_11 (Conv2D)              (None, 100, 100, 32) 9248        activation_9[0][0]               
__________________________________________________________________________________________________
batch_normalization_10 (BatchNo (None, 100, 100, 32) 128         conv2d_11[0][0]                  
__________________________________________________________________________________________________
add_4 (Add)                     (None, 100, 100, 32) 0           activation_8[0][0]               
                                                                 batch_normalization_10[0][0]     
__________________________________________________________________________________________________
activation_10 (Activation)      (None, 100, 100, 32) 0           add_4[0][0]                      
__________________________________________________________________________________________________
conv2d_12 (Conv2D)              (None, 100, 100, 32) 9248        activation_10[0][0]              
__________________________________________________________________________________________________
batch_normalization_11 (BatchNo (None, 100, 100, 32) 128         conv2d_12[0][0]                  
__________________________________________________________________________________________________
activation_11 (Activation)      (None, 100, 100, 32) 0           batch_normalization_11[0][0]     
__________________________________________________________________________________________________
conv2d_13 (Conv2D)              (None, 100, 100, 32) 9248        activation_11[0][0]              
__________________________________________________________________________________________________
batch_normalization_12 (BatchNo (None, 100, 100, 32) 128         conv2d_13[0][0]                  
__________________________________________________________________________________________________
add_5 (Add)                     (None, 100, 100, 32) 0           activation_10[0][0]              
                                                                 batch_normalization_12[0][0]     
__________________________________________________________________________________________________
activation_12 (Activation)      (None, 100, 100, 32) 0           add_5[0][0]                      
__________________________________________________________________________________________________
conv2d_14 (Conv2D)              (None, 50, 50, 64)   18496       activation_12[0][0]              
__________________________________________________________________________________________________
batch_normalization_13 (BatchNo (None, 50, 50, 64)   256         conv2d_14[0][0]                  
__________________________________________________________________________________________________
activation_13 (Activation)      (None, 50, 50, 64)   0           batch_normalization_13[0][0]     
__________________________________________________________________________________________________
conv2d_15 (Conv2D)              (None, 50, 50, 64)   36928       activation_13[0][0]              
__________________________________________________________________________________________________
conv2d_16 (Conv2D)              (None, 50, 50, 64)   2112        activation_12[0][0]              
__________________________________________________________________________________________________
batch_normalization_14 (BatchNo (None, 50, 50, 64)   256         conv2d_15[0][0]                  
__________________________________________________________________________________________________
add_6 (Add)                     (None, 50, 50, 64)   0           conv2d_16[0][0]                  
                                                                 batch_normalization_14[0][0]     
__________________________________________________________________________________________________
activation_14 (Activation)      (None, 50, 50, 64)   0           add_6[0][0]                      
__________________________________________________________________________________________________
conv2d_17 (Conv2D)              (None, 50, 50, 64)   36928       activation_14[0][0]              
__________________________________________________________________________________________________
batch_normalization_15 (BatchNo (None, 50, 50, 64)   256         conv2d_17[0][0]                  
__________________________________________________________________________________________________
activation_15 (Activation)      (None, 50, 50, 64)   0           batch_normalization_15[0][0]     
__________________________________________________________________________________________________
conv2d_18 (Conv2D)              (None, 50, 50, 64)   36928       activation_15[0][0]              
__________________________________________________________________________________________________
batch_normalization_16 (BatchNo (None, 50, 50, 64)   256         conv2d_18[0][0]                  
__________________________________________________________________________________________________
add_7 (Add)                     (None, 50, 50, 64)   0           activation_14[0][0]              
                                                                 batch_normalization_16[0][0]     
__________________________________________________________________________________________________
activation_16 (Activation)      (None, 50, 50, 64)   0           add_7[0][0]                      
__________________________________________________________________________________________________
conv2d_19 (Conv2D)              (None, 50, 50, 64)   36928       activation_16[0][0]              
__________________________________________________________________________________________________
batch_normalization_17 (BatchNo (None, 50, 50, 64)   256         conv2d_19[0][0]                  
__________________________________________________________________________________________________
activation_17 (Activation)      (None, 50, 50, 64)   0           batch_normalization_17[0][0]     
__________________________________________________________________________________________________
conv2d_20 (Conv2D)              (None, 50, 50, 64)   36928       activation_17[0][0]              
__________________________________________________________________________________________________
batch_normalization_18 (BatchNo (None, 50, 50, 64)   256         conv2d_20[0][0]                  
__________________________________________________________________________________________________
add_8 (Add)                     (None, 50, 50, 64)   0           activation_16[0][0]              
                                                                 batch_normalization_18[0][0]     
__________________________________________________________________________________________________
activation_18 (Activation)      (None, 50, 50, 64)   0           add_8[0][0]                      
__________________________________________________________________________________________________
average_pooling2d (AveragePooli (None, 6, 6, 64)     0           activation_18[0][0]              
__________________________________________________________________________________________________
dropout (Dropout)               (None, 6, 6, 64)     0           average_pooling2d[0][0]          
__________________________________________________________________________________________________
flatten (Flatten)               (None, 2304)         0           dropout[0][0]                    
__________________________________________________________________________________________________
dense (Dense)                   (None, 1)            2305        flatten[0][0]                    
==================================================================================================
Total params: 275,809
Trainable params: 274,433
Non-trainable params: 1,376
__________________________________________________________________________________________________
The model name is:  RN20v1_Tr5000_Te2462_bs10_ep050_aug1_VIS1_NIR000_DBraw_ratio0.5_dropout_CORRECT.h5
Using the raw bias (no weights applied).
Using weights: {0: 1.0, 1: 1.0}
Train the ResNet using real-time data augmentation.
Learning rate:  0.001
Epoch 1/50
Epoch 1/50

Epoch 00001: val_acc improved from -inf to 0.52358, saving model to /home/epfl/variu/phd/gravitational_lens_ml/results/checkpoints/resnet/RN20v1_Tr5000_Te2462_bs10_ep050_aug1_VIS1_NIR000_DBraw_ratio0.5_dropout_CORRECT.h5
500/500 - 690s - loss: 0.9014 - tp: 1279.0000 - fp: 1247.0000 - tn: 1253.0000 - fn: 1221.0000 - acc: 0.5064 - auc: 0.5113 - val_loss: 0.8418 - val_tp: 733.0000 - val_fp: 675.0000 - val_tn: 555.0000 - val_fn: 497.0000 - val_acc: 0.5236 - val_auc: 0.5221
Learning rate:  0.001
Epoch 2/50
Epoch 1/50

Epoch 00002: val_acc did not improve from 0.52358
500/500 - 653s - loss: 0.8400 - tp: 1256.0000 - fp: 1256.0000 - tn: 1244.0000 - fn: 1244.0000 - acc: 0.5000 - auc: 0.4993 - val_loss: 0.8273 - val_tp: 966.0000 - val_fp: 915.0000 - val_tn: 315.0000 - val_fn: 264.0000 - val_acc: 0.5207 - val_auc: 0.5165
Learning rate:  0.001
Epoch 3/50
Epoch 1/50

Epoch 00003: val_acc did not improve from 0.52358
500/500 - 655s - loss: 0.8193 - tp: 1252.0000 - fp: 1233.0000 - tn: 1267.0000 - fn: 1248.0000 - acc: 0.5038 - auc: 0.5050 - val_loss: 0.8065 - val_tp: 170.0000 - val_fp: 143.0000 - val_tn: 1087.0000 - val_fn: 1060.0000 - val_acc: 0.5110 - val_auc: 0.5079
Learning rate:  0.001
Epoch 4/50
Epoch 1/50

Epoch 00004: val_acc improved from 0.52358 to 0.53252, saving model to /home/epfl/variu/phd/gravitational_lens_ml/results/checkpoints/resnet/RN20v1_Tr5000_Te2462_bs10_ep050_aug1_VIS1_NIR000_DBraw_ratio0.5_dropout_CORRECT.h5
500/500 - 657s - loss: 0.7980 - tp: 1248.0000 - fp: 1203.0000 - tn: 1297.0000 - fn: 1252.0000 - acc: 0.5090 - auc: 0.5104 - val_loss: 0.7851 - val_tp: 723.0000 - val_fp: 643.0000 - val_tn: 587.0000 - val_fn: 507.0000 - val_acc: 0.5325 - val_auc: 0.5352
Learning rate:  0.001
Epoch 5/50
Epoch 1/50

Epoch 00005: val_acc did not improve from 0.53252
500/500 - 651s - loss: 0.7771 - tp: 1261.0000 - fp: 1201.0000 - tn: 1299.0000 - fn: 1239.0000 - acc: 0.5120 - auc: 0.5230 - val_loss: 0.7695 - val_tp: 227.0000 - val_fp: 195.0000 - val_tn: 1035.0000 - val_fn: 1003.0000 - val_acc: 0.5130 - val_auc: 0.5361
Learning rate:  0.001
Epoch 6/50
Epoch 1/50

Epoch 00006: val_acc did not improve from 0.53252
500/500 - 655s - loss: 0.7614 - tp: 1235.0000 - fp: 1143.0000 - tn: 1357.0000 - fn: 1265.0000 - acc: 0.5184 - auc: 0.5224 - val_loss: 0.7520 - val_tp: 848.0000 - val_fp: 793.0000 - val_tn: 437.0000 - val_fn: 382.0000 - val_acc: 0.5224 - val_auc: 0.5426
Learning rate:  0.001
Epoch 7/50
Epoch 1/50

Epoch 00007: val_acc did not improve from 0.53252
500/500 - 659s - loss: 0.7466 - tp: 1207.0000 - fp: 1154.0000 - tn: 1346.0000 - fn: 1293.0000 - acc: 0.5106 - auc: 0.5216 - val_loss: 0.7407 - val_tp: 487.0000 - val_fp: 410.0000 - val_tn: 820.0000 - val_fn: 743.0000 - val_acc: 0.5313 - val_auc: 0.5393
Learning rate:  0.001
Epoch 8/50
Epoch 1/50

Epoch 00008: val_acc improved from 0.53252 to 0.53862, saving model to /home/epfl/variu/phd/gravitational_lens_ml/results/checkpoints/resnet/RN20v1_Tr5000_Te2462_bs10_ep050_aug1_VIS1_NIR000_DBraw_ratio0.5_dropout_CORRECT.h5
500/500 - 658s - loss: 0.7310 - tp: 1042.0000 - fp: 888.0000 - tn: 1612.0000 - fn: 1458.0000 - acc: 0.5308 - auc: 0.5435 - val_loss: 0.7279 - val_tp: 156.0000 - val_fp: 61.0000 - val_tn: 1169.0000 - val_fn: 1074.0000 - val_acc: 0.5386 - val_auc: 0.5424
Learning rate:  0.001
Epoch 9/50
Epoch 1/50

Epoch 00009: val_acc did not improve from 0.53862
500/500 - 661s - loss: 0.7212 - tp: 1098.0000 - fp: 893.0000 - tn: 1607.0000 - fn: 1402.0000 - acc: 0.5410 - auc: 0.5636 - val_loss: 0.7210 - val_tp: 96.0000 - val_fp: 3.0000 - val_tn: 1227.0000 - val_fn: 1134.0000 - val_acc: 0.5378 - val_auc: 0.5893
Epoch 1/50
Learning rate:  0.001
Epoch 10/50
Epoch 1/50

Epoch 00010: val_acc did not improve from 0.53862
500/500 - 663s - loss: 0.6999 - tp: 1108.0000 - fp: 730.0000 - tn: 1770.0000 - fn: 1392.0000 - acc: 0.5756 - auc: 0.6057 - val_loss: 0.7221 - val_tp: 154.0000 - val_fp: 77.0000 - val_tn: 1153.0000 - val_fn: 1076.0000 - val_acc: 0.5313 - val_auc: 0.5395
Epoch 1/50
Learning rate:  0.001
Epoch 11/50
Epoch 1/50

Epoch 00011: val_acc improved from 0.53862 to 0.58008, saving model to /home/epfl/variu/phd/gravitational_lens_ml/results/checkpoints/resnet/RN20v1_Tr5000_Te2462_bs10_ep050_aug1_VIS1_NIR000_DBraw_ratio0.5_dropout_CORRECT.h5
500/500 - 667s - loss: 0.6984 - tp: 906.0000 - fp: 557.0000 - tn: 1943.0000 - fn: 1594.0000 - acc: 0.5698 - auc: 0.5866 - val_loss: 0.6898 - val_tp: 534.0000 - val_fp: 337.0000 - val_tn: 893.0000 - val_fn: 696.0000 - val_acc: 0.5801 - val_auc: 0.6055
Epoch 1/50
Learning rate:  0.001
Epoch 12/50
Epoch 1/50

Epoch 00012: val_acc improved from 0.58008 to 0.60488, saving model to /home/epfl/variu/phd/gravitational_lens_ml/results/checkpoints/resnet/RN20v1_Tr5000_Te2462_bs10_ep050_aug1_VIS1_NIR000_DBraw_ratio0.5_dropout_CORRECT.h5
500/500 - 663s - loss: 0.6810 - tp: 1045.0000 - fp: 595.0000 - tn: 1905.0000 - fn: 1455.0000 - acc: 0.5900 - auc: 0.6246 - val_loss: 0.6757 - val_tp: 393.0000 - val_fp: 135.0000 - val_tn: 1095.0000 - val_fn: 837.0000 - val_acc: 0.6049 - val_auc: 0.6241
Learning rate:  0.001
Epoch 13/50
Epoch 1/50

Epoch 00013: val_acc did not improve from 0.60488
500/500 - 669s - loss: 0.6790 - tp: 1033.0000 - fp: 580.0000 - tn: 1920.0000 - fn: 1467.0000 - acc: 0.5906 - auc: 0.6211 - val_loss: 0.6820 - val_tp: 650.0000 - val_fp: 432.0000 - val_tn: 798.0000 - val_fn: 580.0000 - val_acc: 0.5886 - val_auc: 0.6209
Learning rate:  0.001
Epoch 14/50
Epoch 1/50

Epoch 00014: val_acc did not improve from 0.60488
500/500 - 664s - loss: 0.6820 - tp: 946.0000 - fp: 503.0000 - tn: 1997.0000 - fn: 1554.0000 - acc: 0.5886 - auc: 0.6184 - val_loss: 0.6851 - val_tp: 263.0000 - val_fp: 88.0000 - val_tn: 1142.0000 - val_fn: 967.0000 - val_acc: 0.5711 - val_auc: 0.6455
Learning rate:  0.001
Epoch 15/50
Epoch 1/50

Epoch 00015: val_acc did not improve from 0.60488
500/500 - 677s - loss: 0.6800 - tp: 1057.0000 - fp: 574.0000 - tn: 1926.0000 - fn: 1443.0000 - acc: 0.5966 - auc: 0.6236 - val_loss: 0.7003 - val_tp: 389.0000 - val_fp: 265.0000 - val_tn: 965.0000 - val_fn: 841.0000 - val_acc: 0.5504 - val_auc: 0.5718
Learning rate:  0.001
Epoch 16/50
Epoch 1/50

Epoch 00016: val_acc improved from 0.60488 to 0.61138, saving model to /home/epfl/variu/phd/gravitational_lens_ml/results/checkpoints/resnet/RN20v1_Tr5000_Te2462_bs10_ep050_aug1_VIS1_NIR000_DBraw_ratio0.5_dropout_CORRECT.h5
500/500 - 662s - loss: 0.6818 - tp: 937.0000 - fp: 489.0000 - tn: 2011.0000 - fn: 1563.0000 - acc: 0.5896 - auc: 0.6149 - val_loss: 0.6664 - val_tp: 391.0000 - val_fp: 117.0000 - val_tn: 1113.0000 - val_fn: 839.0000 - val_acc: 0.6114 - val_auc: 0.6460
Learning rate:  0.001
Epoch 17/50
Epoch 1/50

Epoch 00017: val_acc improved from 0.61138 to 0.61220, saving model to /home/epfl/variu/phd/gravitational_lens_ml/results/checkpoints/resnet/RN20v1_Tr5000_Te2462_bs10_ep050_aug1_VIS1_NIR000_DBraw_ratio0.5_dropout_CORRECT.h5
500/500 - 666s - loss: 0.6720 - tp: 1149.0000 - fp: 581.0000 - tn: 1919.0000 - fn: 1351.0000 - acc: 0.6136 - auc: 0.6512 - val_loss: 0.6784 - val_tp: 526.0000 - val_fp: 250.0000 - val_tn: 980.0000 - val_fn: 704.0000 - val_acc: 0.6122 - val_auc: 0.6284
Epoch 1/50
Learning rate:  0.001
Epoch 18/50
Epoch 1/50

Epoch 00018: val_acc did not improve from 0.61220
500/500 - 665s - loss: 0.6750 - tp: 1065.0000 - fp: 545.0000 - tn: 1955.0000 - fn: 1435.0000 - acc: 0.6040 - auc: 0.6386 - val_loss: 0.7045 - val_tp: 868.0000 - val_fp: 649.0000 - val_tn: 581.0000 - val_fn: 362.0000 - val_acc: 0.5890 - val_auc: 0.6351
Learning rate:  0.001
Epoch 19/50
Epoch 1/50

Epoch 00019: val_acc improved from 0.61220 to 0.61545, saving model to /home/epfl/variu/phd/gravitational_lens_ml/results/checkpoints/resnet/RN20v1_Tr5000_Te2462_bs10_ep050_aug1_VIS1_NIR000_DBraw_ratio0.5_dropout_CORRECT.h5
500/500 - 666s - loss: 0.6717 - tp: 1102.0000 - fp: 554.0000 - tn: 1946.0000 - fn: 1398.0000 - acc: 0.6096 - auc: 0.6426 - val_loss: 0.6572 - val_tp: 415.0000 - val_fp: 131.0000 - val_tn: 1099.0000 - val_fn: 815.0000 - val_acc: 0.6154 - val_auc: 0.6768
Learning rate:  0.001
Epoch 20/50
Epoch 1/50

Epoch 00020: val_acc did not improve from 0.61545
500/500 - 665s - loss: 0.6646 - tp: 1138.0000 - fp: 543.0000 - tn: 1957.0000 - fn: 1362.0000 - acc: 0.6190 - auc: 0.6516 - val_loss: 0.6642 - val_tp: 372.0000 - val_fp: 93.0000 - val_tn: 1137.0000 - val_fn: 858.0000 - val_acc: 0.6134 - val_auc: 0.6595
Learning rate:  0.001
Epoch 21/50
Epoch 1/50

Epoch 00021: val_acc improved from 0.61545 to 0.62358, saving model to /home/epfl/variu/phd/gravitational_lens_ml/results/checkpoints/resnet/RN20v1_Tr5000_Te2462_bs10_ep050_aug1_VIS1_NIR000_DBraw_ratio0.5_dropout_CORRECT.h5
500/500 - 671s - loss: 0.6690 - tp: 1111.0000 - fp: 548.0000 - tn: 1952.0000 - fn: 1389.0000 - acc: 0.6126 - auc: 0.6484 - val_loss: 0.6626 - val_tp: 685.0000 - val_fp: 381.0000 - val_tn: 849.0000 - val_fn: 545.0000 - val_acc: 0.6236 - val_auc: 0.6616
Epoch 1/50
Learning rate:  0.001
Epoch 22/50
Epoch 1/50

Epoch 00022: val_acc did not improve from 0.62358
500/500 - 670s - loss: 0.6615 - tp: 1116.0000 - fp: 516.0000 - tn: 1984.0000 - fn: 1384.0000 - acc: 0.6200 - auc: 0.6616 - val_loss: 0.6907 - val_tp: 493.0000 - val_fp: 314.0000 - val_tn: 916.0000 - val_fn: 737.0000 - val_acc: 0.5728 - val_auc: 0.6068
Learning rate:  0.001
Epoch 23/50
Epoch 1/50

Epoch 00023: val_acc did not improve from 0.62358
500/500 - 679s - loss: 0.6556 - tp: 1185.0000 - fp: 570.0000 - tn: 1930.0000 - fn: 1315.0000 - acc: 0.6230 - auc: 0.6647 - val_loss: 0.6874 - val_tp: 809.0000 - val_fp: 542.0000 - val_tn: 688.0000 - val_fn: 421.0000 - val_acc: 0.6085 - val_auc: 0.6563
Learning rate:  0.001
Epoch 24/50
Epoch 1/50

Epoch 00024: val_acc did not improve from 0.62358
500/500 - 673s - loss: 0.6607 - tp: 1105.0000 - fp: 535.0000 - tn: 1965.0000 - fn: 1395.0000 - acc: 0.6140 - auc: 0.6543 - val_loss: 0.6535 - val_tp: 476.0000 - val_fp: 177.0000 - val_tn: 1053.0000 - val_fn: 754.0000 - val_acc: 0.6215 - val_auc: 0.6746
Learning rate:  0.001
Epoch 25/50
Epoch 1/50

Epoch 00025: val_acc did not improve from 0.62358
500/500 - 670s - loss: 0.6551 - tp: 1173.0000 - fp: 582.0000 - tn: 1918.0000 - fn: 1327.0000 - acc: 0.6182 - auc: 0.6719 - val_loss: 0.6633 - val_tp: 431.0000 - val_fp: 149.0000 - val_tn: 1081.0000 - val_fn: 799.0000 - val_acc: 0.6146 - val_auc: 0.6637
Learning rate:  0.001
Epoch 26/50
Epoch 1/50

Epoch 00026: val_acc did not improve from 0.62358
500/500 - 688s - loss: 0.6636 - tp: 1165.0000 - fp: 578.0000 - tn: 1922.0000 - fn: 1335.0000 - acc: 0.6174 - auc: 0.6650 - val_loss: 0.6686 - val_tp: 662.0000 - val_fp: 415.0000 - val_tn: 815.0000 - val_fn: 568.0000 - val_acc: 0.6004 - val_auc: 0.6435
Learning rate:  0.001
Epoch 27/50
Epoch 1/50

Epoch 00027: val_acc did not improve from 0.62358
500/500 - 685s - loss: 0.6512 - tp: 1216.0000 - fp: 538.0000 - tn: 1962.0000 - fn: 1284.0000 - acc: 0.6356 - auc: 0.6829 - val_loss: 0.6544 - val_tp: 533.0000 - val_fp: 233.0000 - val_tn: 997.0000 - val_fn: 697.0000 - val_acc: 0.6220 - val_auc: 0.6624
Learning rate:  0.001
Epoch 28/50
Epoch 1/50

Epoch 00028: val_acc did not improve from 0.62358
500/500 - 682s - loss: 0.6669 - tp: 1152.0000 - fp: 574.0000 - tn: 1926.0000 - fn: 1348.0000 - acc: 0.6156 - auc: 0.6542 - val_loss: 0.6922 - val_tp: 542.0000 - val_fp: 371.0000 - val_tn: 859.0000 - val_fn: 688.0000 - val_acc: 0.5695 - val_auc: 0.6095
Learning rate:  0.001
Epoch 29/50
Epoch 1/50

Epoch 00029: val_acc improved from 0.62358 to 0.64106, saving model to /home/epfl/variu/phd/gravitational_lens_ml/results/checkpoints/resnet/RN20v1_Tr5000_Te2462_bs10_ep050_aug1_VIS1_NIR000_DBraw_ratio0.5_dropout_CORRECT.h5
500/500 - 677s - loss: 0.6591 - tp: 1236.0000 - fp: 618.0000 - tn: 1882.0000 - fn: 1264.0000 - acc: 0.6236 - auc: 0.6640 - val_loss: 0.6396 - val_tp: 548.0000 - val_fp: 201.0000 - val_tn: 1029.0000 - val_fn: 682.0000 - val_acc: 0.6411 - val_auc: 0.6922
Learning rate:  0.001
Epoch 30/50
Epoch 1/50

Epoch 00030: val_acc did not improve from 0.64106
500/500 - 687s - loss: 0.6546 - tp: 1201.0000 - fp: 550.0000 - tn: 1950.0000 - fn: 1299.0000 - acc: 0.6302 - auc: 0.6707 - val_loss: 0.6753 - val_tp: 644.0000 - val_fp: 366.0000 - val_tn: 864.0000 - val_fn: 586.0000 - val_acc: 0.6130 - val_auc: 0.6616
Learning rate:  0.001
Epoch 31/50
Epoch 1/50

Epoch 00031: val_acc did not improve from 0.64106
500/500 - 678s - loss: 0.6507 - tp: 1220.0000 - fp: 542.0000 - tn: 1958.0000 - fn: 1280.0000 - acc: 0.6356 - auc: 0.6771 - val_loss: 0.6594 - val_tp: 399.0000 - val_fp: 96.0000 - val_tn: 1134.0000 - val_fn: 831.0000 - val_acc: 0.6232 - val_auc: 0.6626
Learning rate:  0.001
Epoch 32/50
Epoch 1/50

Epoch 00032: val_acc did not improve from 0.64106
500/500 - 686s - loss: 0.6562 - tp: 1180.0000 - fp: 541.0000 - tn: 1959.0000 - fn: 1320.0000 - acc: 0.6278 - auc: 0.6743 - val_loss: 0.6574 - val_tp: 479.0000 - val_fp: 180.0000 - val_tn: 1050.0000 - val_fn: 751.0000 - val_acc: 0.6215 - val_auc: 0.6710
Epoch 1/50
Learning rate:  0.001
Epoch 33/50
Epoch 1/50

Epoch 00033: val_acc did not improve from 0.64106
500/500 - 686s - loss: 0.6431 - tp: 1202.0000 - fp: 521.0000 - tn: 1979.0000 - fn: 1298.0000 - acc: 0.6362 - auc: 0.6833 - val_loss: 0.6474 - val_tp: 641.0000 - val_fp: 345.0000 - val_tn: 885.0000 - val_fn: 589.0000 - val_acc: 0.6203 - val_auc: 0.6715
Learning rate:  0.001
Epoch 34/50
Epoch 1/50

Epoch 00034: val_acc did not improve from 0.64106
500/500 - 690s - loss: 0.6557 - tp: 1133.0000 - fp: 502.0000 - tn: 1998.0000 - fn: 1367.0000 - acc: 0.6262 - auc: 0.6680 - val_loss: 0.6556 - val_tp: 399.0000 - val_fp: 89.0000 - val_tn: 1141.0000 - val_fn: 831.0000 - val_acc: 0.6260 - val_auc: 0.6918
Learning rate:  0.001
Epoch 35/50
Epoch 1/50

Epoch 00035: val_acc did not improve from 0.64106
500/500 - 686s - loss: 0.6514 - tp: 1231.0000 - fp: 560.0000 - tn: 1940.0000 - fn: 1269.0000 - acc: 0.6342 - auc: 0.6783 - val_loss: 0.6598 - val_tp: 334.0000 - val_fp: 59.0000 - val_tn: 1171.0000 - val_fn: 896.0000 - val_acc: 0.6118 - val_auc: 0.6789
Learning rate:  0.001
Epoch 36/50
Epoch 1/50

Epoch 00036: val_acc did not improve from 0.64106
500/500 - 710s - loss: 0.6371 - tp: 1238.0000 - fp: 523.0000 - tn: 1977.0000 - fn: 1262.0000 - acc: 0.6430 - auc: 0.6925 - val_loss: 0.6407 - val_tp: 461.0000 - val_fp: 115.0000 - val_tn: 1115.0000 - val_fn: 769.0000 - val_acc: 0.6407 - val_auc: 0.6921
Learning rate:  0.001
Epoch 37/50
Epoch 1/50

Epoch 00037: val_acc did not improve from 0.64106
500/500 - 696s - loss: 0.6411 - tp: 1242.0000 - fp: 514.0000 - tn: 1986.0000 - fn: 1258.0000 - acc: 0.6456 - auc: 0.6965 - val_loss: 0.6757 - val_tp: 249.0000 - val_fp: 29.0000 - val_tn: 1201.0000 - val_fn: 981.0000 - val_acc: 0.5894 - val_auc: 0.6697
Learning rate:  0.001
Epoch 38/50
Epoch 1/50

Epoch 00038: val_acc did not improve from 0.64106
500/500 - 692s - loss: 0.6423 - tp: 1183.0000 - fp: 478.0000 - tn: 2022.0000 - fn: 1317.0000 - acc: 0.6410 - auc: 0.6905 - val_loss: 0.6933 - val_tp: 204.0000 - val_fp: 26.0000 - val_tn: 1204.0000 - val_fn: 1026.0000 - val_acc: 0.5724 - val_auc: 0.6667
Learning rate:  0.001
Epoch 39/50
Epoch 1/50

Epoch 00039: val_acc did not improve from 0.64106
500/500 - 682s - loss: 0.6430 - tp: 1256.0000 - fp: 519.0000 - tn: 1981.0000 - fn: 1244.0000 - acc: 0.6474 - auc: 0.6932 - val_loss: 0.6337 - val_tp: 458.0000 - val_fp: 114.0000 - val_tn: 1116.0000 - val_fn: 772.0000 - val_acc: 0.6398 - val_auc: 0.7080
Learning rate:  0.001
Epoch 40/50
Epoch 1/50

Epoch 00040: val_acc did not improve from 0.64106
500/500 - 690s - loss: 0.6486 - tp: 1200.0000 - fp: 529.0000 - tn: 1971.0000 - fn: 1300.0000 - acc: 0.6342 - auc: 0.6843 - val_loss: 0.6372 - val_tp: 589.0000 - val_fp: 260.0000 - val_tn: 970.0000 - val_fn: 641.0000 - val_acc: 0.6337 - val_auc: 0.6893
Learning rate:  0.001
Epoch 41/50
Epoch 1/50

Epoch 00041: val_acc did not improve from 0.64106
500/500 - 692s - loss: 0.6388 - tp: 1255.0000 - fp: 524.0000 - tn: 1976.0000 - fn: 1245.0000 - acc: 0.6462 - auc: 0.7001 - val_loss: 0.6557 - val_tp: 323.0000 - val_fp: 46.0000 - val_tn: 1184.0000 - val_fn: 907.0000 - val_acc: 0.6126 - val_auc: 0.7081
Learning rate:  0.001
Epoch 42/50
Epoch 1/50

Epoch 00042: val_acc improved from 0.64106 to 0.65407, saving model to /home/epfl/variu/phd/gravitational_lens_ml/results/checkpoints/resnet/RN20v1_Tr5000_Te2462_bs10_ep050_aug1_VIS1_NIR000_DBraw_ratio0.5_dropout_CORRECT.h5
500/500 - 692s - loss: 0.6332 - tp: 1345.0000 - fp: 553.0000 - tn: 1947.0000 - fn: 1155.0000 - acc: 0.6584 - auc: 0.7156 - val_loss: 0.6317 - val_tp: 691.0000 - val_fp: 312.0000 - val_tn: 918.0000 - val_fn: 539.0000 - val_acc: 0.6541 - val_auc: 0.7049
Epoch 1/50
Learning rate:  0.001
Epoch 43/50
Epoch 1/50

Epoch 00043: val_acc did not improve from 0.65407
500/500 - 698s - loss: 0.6355 - tp: 1276.0000 - fp: 501.0000 - tn: 1999.0000 - fn: 1224.0000 - acc: 0.6550 - auc: 0.7055 - val_loss: 0.6413 - val_tp: 441.0000 - val_fp: 91.0000 - val_tn: 1139.0000 - val_fn: 789.0000 - val_acc: 0.6423 - val_auc: 0.7092
Learning rate:  0.001
Epoch 44/50
Epoch 1/50

Epoch 00044: val_acc did not improve from 0.65407
500/500 - 707s - loss: 0.6281 - tp: 1318.0000 - fp: 519.0000 - tn: 1981.0000 - fn: 1182.0000 - acc: 0.6598 - auc: 0.7120 - val_loss: 0.6526 - val_tp: 390.0000 - val_fp: 83.0000 - val_tn: 1147.0000 - val_fn: 840.0000 - val_acc: 0.6248 - val_auc: 0.7050
Learning rate:  0.001
Epoch 45/50
Epoch 1/50

Epoch 00045: val_acc did not improve from 0.65407
500/500 - 698s - loss: 0.6325 - tp: 1274.0000 - fp: 499.0000 - tn: 2001.0000 - fn: 1226.0000 - acc: 0.6550 - auc: 0.7087 - val_loss: 0.6419 - val_tp: 428.0000 - val_fp: 91.0000 - val_tn: 1139.0000 - val_fn: 802.0000 - val_acc: 0.6370 - val_auc: 0.7041
Learning rate:  0.001
Epoch 46/50
Epoch 1/50

Epoch 00046: val_acc did not improve from 0.65407
500/500 - 715s - loss: 0.6320 - tp: 1283.0000 - fp: 494.0000 - tn: 2006.0000 - fn: 1217.0000 - acc: 0.6578 - auc: 0.7139 - val_loss: 0.6501 - val_tp: 347.0000 - val_fp: 45.0000 - val_tn: 1185.0000 - val_fn: 883.0000 - val_acc: 0.6228 - val_auc: 0.7155
Epoch 1/50
Learning rate:  0.001
Epoch 47/50
Epoch 1/50

Epoch 00047: val_acc did not improve from 0.65407
500/500 - 698s - loss: 0.6318 - tp: 1279.0000 - fp: 484.0000 - tn: 2016.0000 - fn: 1221.0000 - acc: 0.6590 - auc: 0.7093 - val_loss: 0.6676 - val_tp: 287.0000 - val_fp: 25.0000 - val_tn: 1205.0000 - val_fn: 943.0000 - val_acc: 0.6065 - val_auc: 0.6871
Learning rate:  0.001
Epoch 48/50
Epoch 1/50

Epoch 00048: val_acc did not improve from 0.65407
500/500 - 713s - loss: 0.6349 - tp: 1322.0000 - fp: 544.0000 - tn: 1956.0000 - fn: 1178.0000 - acc: 0.6556 - auc: 0.7100 - val_loss: 0.6337 - val_tp: 439.0000 - val_fp: 82.0000 - val_tn: 1148.0000 - val_fn: 791.0000 - val_acc: 0.6451 - val_auc: 0.7280
Learning rate:  0.001
Epoch 49/50
Epoch 1/50

Epoch 00049: val_acc did not improve from 0.65407
500/500 - 699s - loss: 0.6273 - tp: 1285.0000 - fp: 470.0000 - tn: 2030.0000 - fn: 1215.0000 - acc: 0.6630 - auc: 0.7193 - val_loss: 0.6343 - val_tp: 737.0000 - val_fp: 382.0000 - val_tn: 848.0000 - val_fn: 493.0000 - val_acc: 0.6443 - val_auc: 0.7053
Learning rate:  0.001
Epoch 50/50
Epoch 1/50

Epoch 00050: val_acc did not improve from 0.65407
500/500 - 703s - loss: 0.6374 - tp: 1267.0000 - fp: 494.0000 - tn: 2006.0000 - fn: 1233.0000 - acc: 0.6546 - auc: 0.7088 - val_loss: 0.6763 - val_tp: 235.0000 - val_fp: 15.0000 - val_tn: 1215.0000 - val_fn: 995.0000 - val_acc: 0.5894 - val_auc: 0.6952
246/246 - 35s - loss: 0.6763 - tp: 235.0000 - fp: 15.0000 - tn: 1215.0000 - fn: 995.0000 - acc: 0.5894 - auc: 0.6952
Test loss: 0.6763308474688026
Test accuracy: 235.0
