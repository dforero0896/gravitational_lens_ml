{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "evalue": "Error: Jupyter server crashed. Unable to connect. \r\nError code from jupyter: 1",
     "output_type": "error"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import pickle\n",
    "import tifffile\n",
    "import sys\n",
    "from astropy.visualization import AsymmetricPercentileInterval, LogStretch, MinMaxInterval\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-26T13:29:22.253331Z",
     "start_time": "2019-11-26T13:29:22.246464Z"
    }
   },
   "outputs": [],
   "source": [
    "WORKDIR='/home/daniel/gdrive/EPFL/2019-2020/MachineLearning/Project/gravitational_lens_ml'\n",
    "SRC = os.path.join(WORKDIR, 'src')\n",
    "DATA = os.path.join(WORKDIR,'data')\n",
    "RESULTS = os.path.join(WORKDIR, 'results')\n",
    "TRAIN = os.path.join(DATA, 'datapack2.0train/Public')\n",
    "TEST = os.path.join(DATA, 'datapack2.0test/Public')\n",
    "TRAIN_MULTIBAND = os.path.join(DATA, 'train_multiband')\n",
    "TRAIN_MULTIBAND_AUGMENT = os.path.join(DATA, 'train_multiband_augment')\n",
    "TEST_MULTIBAND = os.path.join(DATA, 'test_multiband')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-26T13:29:23.004956Z",
     "start_time": "2019-11-26T13:29:22.256275Z"
    }
   },
   "outputs": [],
   "source": [
    "image_catalog = pd.read_csv(os.path.join(DATA, 'catalog/image_catalog2.0train.csv'), comment='#', index_col=0)\n",
    "print(image_catalog.shape)\n",
    "display(image_catalog.isna().sum(axis=0))\n",
    "# No effective magnification for 11182 records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-26T13:29:25.092695Z",
     "start_time": "2019-11-26T13:29:23.007542Z"
    }
   },
   "outputs": [],
   "source": [
    "band = 'EUC_VIS'\n",
    "def get_file_id(filename, delimiters = '_|\\.|-'):\n",
    "    id_ = [int(s) for s in re.split(delimiters, filename) if s.isdigit()][0]\n",
    "    return id_\n",
    "file_id_train = np.array([get_file_id(f) for f in os.listdir(os.path.join(TRAIN, band))], dtype=int)\n",
    "file_id_test = np.array([get_file_id(f) for f in os.listdir(os.path.join(TEST, band))], dtype=int)\n",
    "def check_existing_files(band, set_):\n",
    "    for ID in image_catalog.ID:\n",
    "        if not os.path.isfile(os.path.join(set_, band,'image%s-%i.fits'%(band, ID))):\n",
    "            print('File image%s-%i.fits does not exist in set.'%(band, ID))\n",
    "missing_img =  np.setdiff1d(image_catalog.ID.values, file_id_train, assume_unique=False)\n",
    "print(file_id_train.shape)\n",
    "print(missing_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-26T13:29:25.108232Z",
     "start_time": "2019-11-26T13:29:25.094934Z"
    }
   },
   "outputs": [],
   "source": [
    "image_catalog['is_lens'] = (image_catalog['mag_lens'] > 1.2) & (image_catalog['n_sources'] != 0)\n",
    "print('Number of lenses: %i'%image_catalog['is_lens'].sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-26T13:29:25.132727Z",
     "start_time": "2019-11-26T13:29:25.111059Z"
    }
   },
   "outputs": [],
   "source": [
    "image_catalog['img_exists'] = True\n",
    "image_catalog['img_exists'].loc[image_catalog['ID'].isin(missing_img)] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-26T13:29:25.528573Z",
     "start_time": "2019-11-26T13:29:25.135355Z"
    }
   },
   "outputs": [],
   "source": [
    "image_catalog = image_catalog.drop_duplicates('ID')\n",
    "image_catalog[['ID',\n",
    "               'is_lens']][image_catalog['img_exists']].to_csv(os.path.join(\n",
    "                   RESULTS, 'lens_id_labels.csv'),\n",
    "                                                               index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-26T13:29:26.373989Z",
     "start_time": "2019-11-26T13:29:25.531134Z"
    }
   },
   "outputs": [],
   "source": [
    "import aplpy \n",
    "from astropy.io import fits\n",
    "from astropy.wcs import WCS\n",
    "from reproject import reproject_interp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-26T13:29:26.469100Z",
     "start_time": "2019-11-26T13:29:26.376927Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_image_filename_from_id(id_, band, set_ = TRAIN, ext = 'fits'):\n",
    "    fname = os.path.join(set_, '{0}/image{0}-{1}.{2}'.format(band, id_, ext))\n",
    "    return fname\n",
    "def plot_all_bands_from_id(id_):\n",
    "    fig = plt.figure(figsize=(20, 6))\n",
    "    bands = ['EUC_VIS', 'EUC_H', 'EUC_Y', 'EUC_J']\n",
    "    fitsfigs = []\n",
    "    data = []\n",
    "    for i, band in enumerate(bands):\n",
    "        with fits.open(get_image_filename_from_id(id_, band)) as hdu:\n",
    "            data.append(hdu[0].data)\n",
    "        fitsfigs.append(aplpy.FITSFigure(data[i], figure=fig, subplot=(1,4,i+1)))\n",
    "        fitsfigs[i].show_colorscale()\n",
    "        fitsfigs[i].set_theme('preety')\n",
    "        fitsfigs[i].set_title(band)\n",
    "        fitsfigs[i].add_colorbar()\n",
    "    fig.tight_layout() \n",
    "with fits.open(get_image_filename_from_id(290000, 'EUC_VIS')) as hdu1:\n",
    "    with fits.open(get_image_filename_from_id(290000, 'EUC_H')) as hdu2:\n",
    "        data1 = hdu1[0].data\n",
    "        data2 = hdu2[0].data\n",
    "        data2_reprojected, data2_footprint = reproject_interp(hdu2[0], hdu1[0].header)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-26T13:29:32.120536Z",
     "start_time": "2019-11-26T13:29:26.471112Z"
    }
   },
   "outputs": [],
   "source": [
    "plot_all_bands_from_id(270610)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combine bands into `tiff` and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-26T13:29:32.130203Z",
     "start_time": "2019-11-26T13:29:32.122214Z"
    }
   },
   "outputs": [],
   "source": [
    "def build_image(id_, set_, bands = ['EUC_VIS', 'EUC_H', 'EUC_J', 'EUC_Y'], img_size=200, scale = 100):\n",
    "    tables = []\n",
    "    data = np.empty((img_size, img_size, len(bands)))\n",
    "    for i, band in enumerate(bands):\n",
    "        tables.append(fits.open(get_image_filename_from_id(id_, band)))\n",
    "        if band != 'EUC_VIS':\n",
    "            band_data, data_footprint = reproject_interp(tables[i][0], tables[0][0].header)\n",
    "        else:\n",
    "            band_data = tables[0][0].data\n",
    "        band_data[np.isnan(band_data)] = 0.\n",
    "        interval = AsymmetricPercentileInterval(0, 100, n_samples=100000)\n",
    "        vmin, vmax = interval.get_limits(band_data)\n",
    "        stretch = LogStretch() + MinMaxInterval()\n",
    "        data[:,:,i] = stretch(((np.clip(band_data, vmin*(1), vmax))/(vmax)))\n",
    "    for t in tables:\n",
    "        t.close()\n",
    "    return data.astype(np.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-26T13:29:33.199521Z",
     "start_time": "2019-11-26T13:29:32.132070Z"
    }
   },
   "outputs": [],
   "source": [
    "img = build_image(270210, TRAIN)\n",
    "print(sys.getsizeof(img))\n",
    "for i in range(4):\n",
    "    plt.figure()\n",
    "    print(np.min(img[:,:,i]), np.max(img[:,:,i]))\n",
    "    a = plt.imshow((img[:,:,i]))\n",
    "print(image_catalog.loc[image_catalog['ID']==270210]['is_lens'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare preprocessing with Log + Norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-26T13:29:33.210477Z",
     "start_time": "2019-11-26T13:29:33.202043Z"
    }
   },
   "outputs": [],
   "source": [
    "def build_image_noprocess(id_, set_, bands = ['EUC_VIS', 'EUC_H', 'EUC_J', 'EUC_Y'], img_size=200, scale = 100):\n",
    "    tables = []\n",
    "    data = np.empty((img_size, img_size, len(bands)))\n",
    "    for i, band in enumerate(bands):\n",
    "        tables.append(fits.open(get_image_filename_from_id(id_, band)))\n",
    "        if band != 'EUC_VIS':\n",
    "            band_data, data_footprint = reproject_interp(tables[i][0], tables[0][0].header)\n",
    "        else:\n",
    "            band_data = tables[0][0].data\n",
    "        band_data[np.isnan(band_data)] = 0.\n",
    "        norm =  LogStretch() + MinMaxInterval()\n",
    "        data[:,:,i] = norm(band_data)\n",
    "    for t in tables:\n",
    "        t.close()\n",
    "    return data.astype(np.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-26T13:29:33.665659Z",
     "start_time": "2019-11-26T13:29:33.213320Z"
    }
   },
   "outputs": [],
   "source": [
    "test_id = 270210\n",
    "img = build_image(test_id, TRAIN)\n",
    "img_nopr = build_image_noprocess(test_id, TRAIN)\n",
    "def plot_slice(image, slice_ratio, axis=0):\n",
    "    '''Plot a single slice of a picture.\n",
    "    \n",
    "    image: Image to plot.\n",
    "    slice_ratio: float from 0 to 1 where 0 is the top slice, and 1 is the last slice.\n",
    "    axis: 0 or 1 to show horizontal or vertical slices'''\n",
    "    \n",
    "    index = int(image.shape[axis] * slice_ratio)\n",
    "    slice_ = image[index]\n",
    "    plt.plot(slice_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-26T13:29:34.018565Z",
     "start_time": "2019-11-26T13:29:33.667623Z"
    }
   },
   "outputs": [],
   "source": [
    "plot_slice(img, 0.5)\n",
    "plt.figure()\n",
    "plot_slice(img_nopr, 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-26T13:29:34.024957Z",
     "start_time": "2019-11-26T13:29:34.020730Z"
    }
   },
   "outputs": [],
   "source": [
    "def plot_slice_stack(image, axis=0):\n",
    "    '''Plot a single slice of a picture.\n",
    "    \n",
    "    image: Image to plot.'''\n",
    "    \n",
    "    slice_ = np.sum(image, axis=axis)\n",
    "    print(slice_.shape)\n",
    "    plt.plot(slice_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-26T13:29:34.454924Z",
     "start_time": "2019-11-26T13:29:34.027883Z"
    }
   },
   "outputs": [],
   "source": [
    "axis = 0\n",
    "plot_slice_stack(img, axis = axis)\n",
    "plt.figure()\n",
    "plot_slice_stack(img_nopr, axis = axis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-26T13:29:38.743273Z",
     "start_time": "2019-11-26T13:29:34.457058Z"
    }
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"-1\"    \n",
    "import tensorflow as tf\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
    "tf.debugging.set_log_device_placement(True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-26T13:29:38.797868Z",
     "start_time": "2019-11-26T13:29:38.748730Z"
    }
   },
   "outputs": [],
   "source": [
    "img = tifffile.imread(DATA+'/train_multiband/image_299896_multiband.tiff')\n",
    "def flatten_by_channel(original_image):\n",
    "    \"\"\"preprocess the image.\"\"\"\n",
    "    n_channels = original_image.shape[-1]\n",
    "    processed_image = original_image.reshape(-1,n_channels)\n",
    "    return processed_image.astype(np.float32) #Overflow if using unsigned int\n",
    "flat_img = flatten_by_channel(img)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-26T13:29:39.489690Z",
     "start_time": "2019-11-26T13:29:38.802879Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "pca = PCA(whiten=False)\n",
    "std_flat_img = scaler.fit_transform(flat_img)\n",
    "pca.fit(std_flat_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-26T13:29:39.689656Z",
     "start_time": "2019-11-26T13:29:39.491370Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.bar(range(len(pca.explained_variance_ratio_)), pca.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-26T13:29:39.700567Z",
     "start_time": "2019-11-26T13:29:39.691461Z"
    }
   },
   "outputs": [],
   "source": [
    "pca_flat_img = pca.fit_transform(std_flat_img)\n",
    "pca_image = pca_flat_img.reshape((img.shape[0], img.shape[1], -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-26T13:29:40.245255Z",
     "start_time": "2019-11-26T13:29:39.705921Z"
    }
   },
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(2, 2, figsize = (7, 7))\n",
    "for i, a in enumerate(ax.ravel()):\n",
    "    a.imshow(pca_image[:,:,i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Covariance Matrix between colors\n",
    "See, that it is almost diagonal. Not surprising that PC's are almost the same as the priginal colors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-26T13:29:40.425374Z",
     "start_time": "2019-11-26T13:29:40.247816Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.imshow(np.cov(flat_img.T))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test  CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-26T13:29:40.443176Z",
     "start_time": "2019-11-26T13:29:40.427231Z"
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Conv2D, Flatten, Dropout, MaxPooling2D\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator, Iterator\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-26T13:29:40.448577Z",
     "start_time": "2019-11-26T13:29:40.445185Z"
    }
   },
   "outputs": [],
   "source": [
    "# From Tensorflow examples\n",
    "batch_size = 24\n",
    "epochs = 15\n",
    "IMG_HEIGHT = 200\n",
    "IMG_WIDTH = 200\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-26T13:29:43.936178Z",
     "start_time": "2019-11-26T13:29:40.450638Z"
    }
   },
   "outputs": [],
   "source": [
    "lens_df = pd.read_csv(os.path.join(RESULTS, 'lens_id_labels.csv'), index_col=0)\n",
    "n_total_clean = len(lens_df)\n",
    "n_lens_clean = len(lens_df[lens_df['is_lens'] == True])\n",
    "n_nolens_clean = len(lens_df[lens_df['is_lens'] == False])\n",
    "equal_class_coeff = n_lens_clean/n_nolens_clean\n",
    "natural_class_coeff = 1e3*n_lens_clean/n_nolens_clean\n",
    "print('Original number of data points: %i' % n_total_clean)\n",
    "print('Original number of lenses: %i' % n_lens_clean)\n",
    "print('Original number of non-lenses: %i' % n_nolens_clean)\n",
    "print('Class 0 (non-lens) should be weighted by a coefficient %.2f to simulate equal number of data points in each class'%(equal_class_coeff))\n",
    "print('Class 0 (non-lens) should be weighted by a coefficient %.2f to simulate natural (1000x more non-lenses) number of data points in each class'%(natural_class_coeff))\n",
    "\n",
    "\n",
    "def build_generator_dataframe(id_label_df, directory=TRAIN_MULTIBAND):\n",
    "    files = os.listdir(directory)\n",
    "    ids = [get_file_id(filename) for filename in files]\n",
    "    df = pd.DataFrame()\n",
    "    df['filenames'] = [\n",
    "        os.path.realpath(os.path.join(directory, f)) for f in files\n",
    "    ]\n",
    "    df['labels'] = id_label_df.loc[ids, 'is_lens'].values.astype(int)\n",
    "    df['ID'] = ids\n",
    "    return df\n",
    "\n",
    "\n",
    "dataframe_for_generator = build_generator_dataframe(lens_df, TRAIN_MULTIBAND)\n",
    "# Append the extra non-lens images:\n",
    "nolens_extra = list(\n",
    "    map(lambda f: os.path.realpath(os.path.join(TRAIN_MULTIBAND_AUGMENT, f)),\n",
    "        os.listdir(TRAIN_MULTIBAND_AUGMENT)))\n",
    "nolens_extra_df = pd.DataFrame(\n",
    "    dict(\n",
    "        zip(['filenames', 'labels', 'ID'], [\n",
    "            nolens_extra,\n",
    "            np.zeros(len(nolens_extra), dtype=int),\n",
    "            9999 * np.ones(len(nolens_extra), dtype=int)\n",
    "        ])))\n",
    "#dataframe_for_generator = pd.concat([dataframe_for_generator,\n",
    "#                                     nolens_extra_df]).sample(frac=1)\n",
    "display(dataframe_for_generator.head())\n",
    "\n",
    "train_df, val_df = train_test_split(dataframe_for_generator,\n",
    "                                    test_size=0.1,\n",
    "                                    random_state=42)\n",
    "total_train = len(train_df)\n",
    "total_val = len(val_df)\n",
    "print('Augmented number of data points: %i' % len(dataframe_for_generator))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-26T13:29:44.488361Z",
     "start_time": "2019-11-26T13:29:43.938092Z"
    }
   },
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    Conv2D(16, 3, padding='same', activation='relu', \n",
    "           input_shape=(IMG_HEIGHT, IMG_WIDTH ,4)),\n",
    "    MaxPooling2D(),\n",
    "    Dropout(0.2),\n",
    "    Conv2D(32, 3, padding='same', activation='relu'),\n",
    "    MaxPooling2D(),\n",
    "    Conv2D(64, 3, padding='same', activation='relu'),\n",
    "    MaxPooling2D(),\n",
    "    Dropout(0.2),\n",
    "    Flatten(),\n",
    "    Dense(512, activation='relu'),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-26T13:29:44.600862Z",
     "start_time": "2019-11-26T13:29:44.491147Z"
    }
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-26T13:29:44.634068Z",
     "start_time": "2019-11-26T13:29:44.604463Z"
    }
   },
   "outputs": [],
   "source": [
    "class TiffImageDataGenerator(ImageDataGenerator):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(TiffImageDataGenerator, self).__init__(*args, **kwargs)\n",
    "\n",
    "    def get_input(self, path):\n",
    "        img = tifffile.imread(path)\n",
    "        return img\n",
    "\n",
    "    def image_generator_dataframe(self,\n",
    "                                  dataframe,\n",
    "                                  directory='',\n",
    "                                  x_col='filename',\n",
    "                                  y_col='class',\n",
    "                                  batch_size=64,\n",
    "                                  validation=False,\n",
    "                                  bands=[True, True, True, True]):\n",
    "        \"\"\"Loads tiff image data by batches and automatically applies transformations.\n",
    "\n",
    "        param: dataframe (pandas.DataFrame): Dataframe containing columns 'filename' and 'class'.\n",
    "        param: directory (str): Path to the directory containing filenames in column 'filename'.\n",
    "        param: x_col (str): Column name of the column containing image filenames. Defaults to 'filenames'.\n",
    "        param: y_col (str): Column name of the column containing image classes. Defaults to 'class'.\n",
    "        param: batch_size (int): Number of images to load at a time. Defaults to 64.\n",
    "        param: validation (bool): Whether or not the generator is used for validation. If True, no transformations are applied.\n",
    "                                    Defaults to False.\n",
    "        param: bands (list of bool): Boolean mask of channels to use. Defaults to [True, True, True, True] (use all channels).\n",
    "        yields: batch_x, batch_y\n",
    "        \"\"\"\n",
    "\n",
    "        files = dataframe[x_col].values\n",
    "        while True:\n",
    "            # Select files (paths/indices) for the batch\n",
    "            batch_paths = np.random.choice(a=files, size=batch_size)\n",
    "            batch_input = []\n",
    "            batch_output = []\n",
    "\n",
    "            # Read in each input, perform preprocessing and get labels\n",
    "            for input_path in batch_paths:\n",
    "                input = self.get_input(os.path.join(directory,\n",
    "                                                    input_path))[:, :, bands]\n",
    "                output = dataframe[dataframe[x_col] ==\n",
    "                                   input_path][y_col].values[0]\n",
    "                if self.preprocessing_function:\n",
    "                    input = self.preprocessing_function(input)\n",
    "                if not validation:\n",
    "                    input = self.random_transform(input)\n",
    "                batch_input += [input]\n",
    "                batch_output += [output]\n",
    "            # Return a tuple of (input,output) to feed the network\n",
    "            batch_x = np.array(batch_input)\n",
    "            batch_y = np.array(batch_output)\n",
    "\n",
    "            yield (batch_x, batch_y)\n",
    "\n",
    "    def prop_image_generator_dataframe(self,\n",
    "                                       dataframe,\n",
    "                                       directory='',\n",
    "                                       x_col='filename',\n",
    "                                       y_col='class',\n",
    "                                       batch_size=64,\n",
    "                                       validation=False,\n",
    "                                       bands=[True, True, True, True],\n",
    "                                       ratio=0.5):\n",
    "        \"\"\"Loads tiff image data by batches and automatically applies transformations. Forces the\n",
    "        proportion of positive/negative to be ratio.\n",
    "\n",
    "        param: dataframe (pandas.DataFrame): Dataframe containing columns 'filename' and 'class'.\n",
    "        param: directory (str): Path to the directory containing filenames in column 'filename'.\n",
    "        param: x_col (str): Column name of the column containing image filenames. Defaults to 'filenames'.\n",
    "        param: y_col (str): Column name of the column containing image classes. Defaults to 'class'.\n",
    "        param: batch_size (int): Number of images to load at a time. Defaults to 64.\n",
    "        param: validation (bool): Whether or not the generator is used for validation. If True, no transformations are applied.\n",
    "                                    Defaults to False.\n",
    "        param: bands (list of bool): Boolean mask of channels to use. Defaults to [True, True, True, True] (use all channels).\n",
    "        param: ratio (float): Ratio positive/negative to force in each batch.\n",
    "        yields: batch_x, batch_y\n",
    "        \"\"\"\n",
    "        lens_df = dataframe[dataframe[y_col] == 1]\n",
    "        nonlens_df = dataframe[dataframe[y_col] == 0]\n",
    "        lens_size = int(ratio * batch_size)\n",
    "        nonlens_size = batch_size - lens_size\n",
    "        while True:\n",
    "            # Select files (paths/indices) for the batch\n",
    "            batch_paths_lens = np.random.choice(a=lens_df[x_col].values,\n",
    "                                                size=lens_size)\n",
    "            batch_paths_nonlens = np.random.choice(\n",
    "                a=nonlens_df[x_col].values, size=nonlens_size)\n",
    "            batch_paths = np.concatenate(\n",
    "                (batch_paths_lens, batch_paths_nonlens)).reshape(\n",
    "                    (lens_size + nonlens_size))\n",
    "            batch_input = []\n",
    "            batch_output = []\n",
    "\n",
    "            # Read in each input, perform preprocessing and get labels\n",
    "            for input_path in batch_paths:\n",
    "                input = self.get_input(os.path.join(\n",
    "                    directory, input_path))[:, :, bands]\n",
    "                output = dataframe[dataframe[x_col] ==\n",
    "                                   input_path][y_col].values[0]\n",
    "                if self.preprocessing_function:\n",
    "                    input = self.preprocessing_function(input)\n",
    "                if not validation:\n",
    "                    input = self.random_transform(input)\n",
    "                batch_input += [input]\n",
    "                batch_output += [output]\n",
    "            # Return a tuple of (input,output) to feed the network\n",
    "            batch_x = np.array(batch_input)\n",
    "            batch_y = np.array(batch_output)\n",
    "\n",
    "            yield (batch_x, batch_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-26T13:29:44.646020Z",
     "start_time": "2019-11-26T13:29:44.636682Z"
    }
   },
   "outputs": [],
   "source": [
    "image_data_gen_train = TiffImageDataGenerator(featurewise_center=False,\n",
    "                                          samplewise_center=False,\n",
    "                                          featurewise_std_normalization=False,\n",
    "                                          samplewise_std_normalization=False,\n",
    "                                          zca_whitening=False,\n",
    "                                          zca_epsilon=1e-06,\n",
    "                                          rotation_range=10,\n",
    "                                          width_shift_range=0.0,\n",
    "                                          height_shift_range=0.0,\n",
    "                                          brightness_range=(0.8, 1.1),\n",
    "                                          shear_range=0.0,\n",
    "                                          zoom_range=(0.9, 1.1),\n",
    "                                          channel_shift_range=0.0,\n",
    "                                          fill_mode='wrap',\n",
    "                                          cval=0.0,\n",
    "                                          horizontal_flip=True,\n",
    "                                          vertical_flip=True,\n",
    "                                          rescale=None,\n",
    "                                          preprocessing_function=None,\n",
    "                                          data_format='channels_last',\n",
    "                                          dtype='float32')\n",
    "image_data_gen_val = TiffImageDataGenerator(dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-26T13:29:45.743843Z",
     "start_time": "2019-11-26T13:29:44.650662Z"
    }
   },
   "outputs": [],
   "source": [
    "train_data_gen = image_data_gen_train.image_generator_dataframe(train_df,\n",
    "                                  directory='',\n",
    "                                  x_col='filenames',\n",
    "                                 y_col='labels', batch_size = 1, validation=False, bands = [True, False, False, False])\n",
    "val_data_gen = image_data_gen_val.image_generator_dataframe(val_df,\n",
    "                                  directory='',\n",
    "                                  x_col='filenames',\n",
    "                                 y_col='labels', batch_size = 1, validation=True)\n",
    "fig, ax = plt.subplots(1,6, figsize = (15, 2.5))\n",
    "for a in ax.ravel():\n",
    "    img, label = next(train_data_gen)\n",
    "    print(img.shape)\n",
    "    a.imshow(img[0][:,:,0])\n",
    "    a.set_xlabel(label[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-26T13:29:46.737296Z",
     "start_time": "2019-11-26T13:29:45.746113Z"
    }
   },
   "outputs": [],
   "source": [
    "train_data_gen = image_data_gen_train.prop_image_generator_dataframe(train_df,\n",
    "                                  directory='',\n",
    "                                  x_col='filenames',\n",
    "                                 y_col='labels', batch_size = 1, validation=False, bands = [True, False, False, False])\n",
    "val_data_gen = image_data_gen_val.prop_image_generator_dataframe(val_df,\n",
    "                                  directory='',\n",
    "                                  x_col='filenames',\n",
    "                                 y_col='labels', batch_size = 1, validation=True)\n",
    "fig, ax = plt.subplots(1,10, figsize = (15, 2.5))\n",
    "for a in ax.ravel():\n",
    "    img, label = next(train_data_gen)\n",
    "    print(img.shape)\n",
    "    a.imshow(img[0][:,:,0])\n",
    "    a.set_xlabel(label[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Augment non-lens sample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-26T13:29:46.751420Z",
     "start_time": "2019-11-26T13:29:46.739019Z"
    }
   },
   "outputs": [],
   "source": [
    "no_lens_df = dataframe_for_generator[dataframe_for_generator['labels'] == 0]\n",
    "display(no_lens_df.head())\n",
    "print('Non-lens original sample size: %s'%len(no_lens_df))\n",
    "print('Total original sample size: %s'%len(lens_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-26T13:29:46.762003Z",
     "start_time": "2019-11-26T13:29:46.753532Z"
    }
   },
   "outputs": [],
   "source": [
    "augment_nolens = TiffImageDataGenerator(featurewise_center=False,\n",
    "                                          samplewise_center=False,\n",
    "                                          featurewise_std_normalization=False,\n",
    "                                          samplewise_std_normalization=False,\n",
    "                                          zca_whitening=False,\n",
    "                                          zca_epsilon=1e-06,\n",
    "                                          rotation_range=20,\n",
    "                                          width_shift_range=0.0,\n",
    "                                          height_shift_range=0.0,\n",
    "                                          brightness_range=(0.8, 1.1),\n",
    "                                          shear_range=0.0,\n",
    "                                          zoom_range=(0.9, 1),\n",
    "                                          channel_shift_range=0.0,\n",
    "                                          fill_mode='wrap',\n",
    "                                          cval=0.0,\n",
    "                                          horizontal_flip=True,\n",
    "                                          vertical_flip=True,\n",
    "                                          rescale=None,\n",
    "                                          preprocessing_function=None,\n",
    "                                          data_format='channels_last',\n",
    "                                          dtype='float32')\n",
    "augment_nolens_gen = augment_nolens.image_generator_dataframe(no_lens_df,\n",
    "                                  directory='',\n",
    "                                  x_col='filenames',\n",
    "                                 y_col='labels', batch_size = 1, validation=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-26T13:29:48.206796Z",
     "start_time": "2019-11-26T13:29:46.765381Z"
    }
   },
   "outputs": [],
   "source": [
    "i = 8\n",
    "fig, ax = plt.subplots(1,i, figsize = (2.5*i, 2.5))\n",
    "for a in ax.ravel():\n",
    "    img, label = next(augment_nolens_gen)\n",
    "    a.imshow(img[0][:,:,3])\n",
    "    a.set_xlabel(label[0])\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load saved model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-26T13:29:50.002732Z",
     "start_time": "2019-11-26T13:29:48.208313Z"
    }
   },
   "outputs": [],
   "source": [
    "new_model = tf.keras.models.load_model(os.path.join(RESULTS, 'simple_cnn.h5'))\n",
    "\n",
    "# Check its architecture\n",
    "new_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-26T13:30:09.293799Z",
     "start_time": "2019-11-26T13:29:50.006049Z"
    }
   },
   "outputs": [],
   "source": [
    "loss, acc = new_model.evaluate_generator(val_data_gen, steps = 1000, use_multiprocessing=True, verbose  = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-26T13:30:09.296821Z",
     "start_time": "2019-11-26T13:29:20.443Z"
    }
   },
   "outputs": [],
   "source": [
    "predictions = new_model.predict_generator(augment_nolens_gen, steps = 10)\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-26T13:30:09.297901Z",
     "start_time": "2019-11-26T13:29:20.449Z"
    }
   },
   "outputs": [],
   "source": [
    "next(augment_nolens_gen)[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-26T13:30:09.299115Z",
     "start_time": "2019-11-26T13:29:20.455Z"
    }
   },
   "outputs": [],
   "source": [
    "random_prediction = new_model.predict(np.random.random(200*200*4).reshape(200, 200, 4)[None, :, :, :])\n",
    "print('The prediction on random noise is: %.2f'%random_prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check History of model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-26T13:31:43.592949Z",
     "start_time": "2019-11-26T13:31:42.932299Z"
    }
   },
   "outputs": [],
   "source": [
    "history_path = os.path.join(RESULTS,\n",
    "                            'gravlens_ResNet20v1_model.epoch025.history')\n",
    "with open(history_path, 'rb') as handle:\n",
    "    history = pickle.load(handle)\n",
    "fig, ax1 = plt.subplots(1, 1, figsize=(10, 5))\n",
    "ax2 = ax1.twinx()\n",
    "ax1.plot(range(len(history['loss'])),\n",
    "         history['val_loss'],\n",
    "         label='Validation loss',\n",
    "         marker='o',\n",
    "         c='b')\n",
    "ax1.plot(range(len(history['loss'])),\n",
    "         history['loss'],\n",
    "         label='Training loss',\n",
    "         marker='o',\n",
    "         c='r')\n",
    "ax2.plot(range(len(history['loss'])),\n",
    "         history['val_acc'],\n",
    "         label='Validation accuracy',\n",
    "         marker='o',\n",
    "         c='b',\n",
    "         ls='--',\n",
    "         fillstyle='none')\n",
    "ax2.plot(range(len(history['loss'])),\n",
    "         history['acc'],\n",
    "         label='Training accuracy',\n",
    "         marker='o',\n",
    "         c='r',\n",
    "         ls='--',\n",
    "         fillstyle='none')\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.legend(loc=(-0.1, 1))\n",
    "ax2.legend(loc=(0.9, 1))\n",
    "ax1.set_ylabel('Loss')\n",
    "ax2.set_ylabel('Accuracy')\n",
    "ax2.set_ylim(0,1)\n",
    "plt.gcf()\n",
    "plt.savefig(os.path.join(\n",
    "    RESULTS,\n",
    "    'plots/' + os.path.basename(history_path).replace('.history', '.png')),\n",
    "            dpi=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-26T13:34:19.894241Z",
     "start_time": "2019-11-26T13:34:19.605211Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "train_tpr = np.array(history['tp'])/(np.array(history['tp']) + np.array(history['fn']))\n",
    "train_fpr = np.array(history['fp'])/(np.array(history['tn']) + np.array(history['fp']))\n",
    "val_tpr = np.array(history['val_tp'])/(np.array(history['val_tp']) + np.array(history['val_fn']))\n",
    "val_fpr = np.array(history['val_fp'])/(np.array(history['val_tn']) + np.array(history['val_fp']))\n",
    "plt.plot(train_fpr, train_tpr, 'ob', label = 'Train')\n",
    "plt.plot(val_fpr, val_tpr, 'or', label = 'Validation')\n",
    "plt.xlabel('FPR')\n",
    "plt.ylabel('TPR')\n",
    "plt.xlim(0,1)\n",
    "plt.ylim(0,1)\n",
    "plt.plot([0,1], [0,1])\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-26T13:35:31.490901Z",
     "start_time": "2019-11-26T13:35:31.486762Z"
    }
   },
   "outputs": [],
   "source": [
    "history.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}