@article{Tompson2015,
abstract = {Recent state-of-the-art performance on human-body pose estimation has been achieved with Deep Convolutional Networks (ConvNets). Traditional ConvNet architectures include pooling and sub-sampling layers which reduce computational requirements, introduce invariance and prevent over-training. These benefits of pooling come at the cost of reduced localization accuracy. We introduce a novel architecture which includes an efficient 'position refinement' model that is trained to estimate the joint offset location within a small region of the image. This refinement model is jointly trained in cascade with a state-of-the-art ConvNet model [21] to achieve improved accuracy in human joint location estimation. We show that the variance of our detector approaches the variance of human annotations on the FLIC [20] dataset and outperforms all existing approaches on the MPII-human-pose dataset [1].},
archivePrefix = {arXiv},
arxivId = {1411.4280},
author = {Tompson, Jonathan and Goroshin, Ross and Jain, Arjun and LeCun, Yann and Bregler, Christoph},
doi = {10.1109/CVPR.2015.7298664},
eprint = {1411.4280},
file = {:home/daniel/Desktop/TOREAD/1411.4280.pdf:pdf},
isbn = {9781467369640},
issn = {10636919},
journal = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
pages = {648--656},
title = {{Efficient object localization using Convolutional Networks}},
volume = {07-12-June-2015},
year = {2015}
}
@article{He2016a,
abstract = {Deep residual networks have emerged as a family of extremely deep architectures showing compelling accuracy and nice convergence behaviors. In this paper, we analyze the propagation formulations behind the residual building blocks, which suggest that the forward and backward signals can be directly propagated from one block to any other block, when using identity mappings as the skip connections and after-addition activation. A series of ablation experiments support the importance of these identity mappings. This motivates us to propose a new residual unit, which makes training easier and improves generalization. We report improved results using a 1001-layer ResNet on CIFAR- 10 (4.62{\%} error) and CIFAR-100, and a 200-layer ResNet on ImageNet.},
archivePrefix = {arXiv},
arxivId = {1603.05027},
author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
doi = {10.1007/978-3-319-46493-0_38},
eprint = {1603.05027},
file = {:home/daniel/OneDrive/Project{\_}bib/ML{\_}p2{\_}bib/resnet{\_}paper2.pdf:pdf},
isbn = {9783319464923},
issn = {16113349},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
pages = {630--645},
title = {{Identity mappings in deep residual networks}},
volume = {9908 LNCS},
year = {2016}
}
@article{He2016,
abstract = {Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers - 8Ã— deeper than VGG nets [40] but still having lower complexity. An ensemble of these residual nets achieves 3.57{\%} error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28{\%} relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC {\&} COCO 2015 competitions1, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.},
archivePrefix = {arXiv},
arxivId = {1512.03385},
author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
doi = {10.1109/CVPR.2016.90},
eprint = {1512.03385},
file = {:home/daniel/OneDrive/Project{\_}bib/ML{\_}p2{\_}bib/resnet{\_}paper.pdf:pdf},
isbn = {9781467388504},
issn = {10636919},
journal = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
pages = {770--778},
title = {{Deep residual learning for image recognition}},
volume = {2016-December},
year = {2016}
}
@misc{Metcalf,
author = {Metcalf, R B},
file = {:home/daniel/Desktop/TOREAD/evaluation.pdf:pdf},
pages = {1--3},
title = {{Scoring of Strong Gravitational Lens Finding Challenge}},
url = {http://metcalf1.difa.unibo.it/DATA3/evaluation.pdf}
}
@article{Schaefer2017,
abstract = {Future large-scale surveys with high resolution imaging will provide us with a few {\$}10{\^{}}5{\$} new strong galaxy-scale lenses. These strong lensing systems however will be contained in large data amounts which are beyond the capacity of human experts to visually classify in a unbiased way. We present a new strong gravitational lens finder based on convolutional neural networks (CNNs). The method was applied to the Strong Lensing challenge organised by the Bologna Lens Factory. It achieved first and third place respectively on the space-based data-set and the ground-based data-set. The goal was to find a fully automated lens finder for ground-based and space-based surveys which minimizes human inspect. We compare the results of our CNN architecture and three new variations ("invariant" "views" and "residual") on the simulated data of the challenge. Each method has been trained separately 5 times on 17 000 simulated images, cross-validated using 3 000 images and then applied to a 100 000 image test set. We used two different metrics for evaluation, the area under the receiver operating characteristic curve (AUC) score and the recall with no false positive ({\$}\backslashmathrm{\{}Recall{\}}{\_}{\{}\backslashmathrm{\{}0FP{\}}{\}}{\$}). For ground based data our best method achieved an AUC score of {\$}0.977{\$} and a {\$}\backslashmathrm{\{}Recall{\}}{\_}{\{}\backslashmathrm{\{}0FP{\}}{\}}{\$} of {\$}0.50{\$}. For space-based data our best method achieved an AUC score of {\$}0.940{\$} and a {\$}\backslashmathrm{\{}Recall{\}}{\_}{\{}\backslashmathrm{\{}0FP{\}}{\}}{\$} of {\$}0.32{\$}. On space-based data adding dihedral invariance to the CNN architecture diminished the overall score but achieved a higher no contamination recall. We found that using committees of 5 CNNs produce the best recall at zero contamination and consistenly score better AUC than a single CNN. We found that for every variation of our CNN lensfinder, we achieve AUC scores close to {\$}1{\$} within {\$}6\backslash{\%}{\$}.},
archivePrefix = {arXiv},
arxivId = {1705.07132},
author = {Schaefer, C. and Geiger, M. and Kuntzer, T. and Kneib, J-P.},
doi = {10.1051/0004-6361/201731201},
eprint = {1705.07132},
file = {:home/daniel/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Schaefer et al. - 2017 - Deep Convolutional Neural Networks as strong gravitational lens detectors.pdf:pdf},
month = {may},
title = {{Deep Convolutional Neural Networks as strong gravitational lens detectors}},
url = {http://arxiv.org/abs/1705.07132 http://dx.doi.org/10.1051/0004-6361/201731201},
year = {2017}
}
@article{Jackson2008,
author = {Jackson, Neal},
doi = {10.1111/j.1365-2966.2008.13629.x},
issn = {00358711},
journal = {Monthly Notices of the Royal Astronomical Society},
month = {sep},
number = {3},
pages = {1311--1318},
title = {{Gravitational lenses and lens candidates identified from the COSMOS field}},
url = {https://academic.oup.com/mnras/article-lookup/doi/10.1111/j.1365-2966.2008.13629.x},
volume = {389},
year = {2008}
}
@inproceedings{Kingma2015,
abstract = {We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.},
archivePrefix = {arXiv},
arxivId = {1412.6980},
author = {Kingma, Diederik P. and Ba, Jimmy Lei},
booktitle = {3rd International Conference on Learning Representations, ICLR 2015 - Conference Track Proceedings},
eprint = {1412.6980},
file = {:home/daniel/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Kingma, Ba - 2015 - Adam A method for stochastic optimization.pdf:pdf},
publisher = {International Conference on Learning Representations, ICLR},
title = {{Adam: A method for stochastic optimization}},
year = {2015}
}
@article{Fukushima1980,
abstract = {A neural network model for a mechanism of visual pattern recognition is proposed in this paper. The network is self-organized by "learning without a teacher", and acquires an ability to recognize stimulus patterns based on the geometrical similarity (Gestalt) of their shapes without affected by their positions. This network is given a nickname "neocognitron". After completion of self-organization, the network has a structure similar to the hierarchy model of the visual nervous system proposed by Hubel and Wiesel. The network consits of an input layer (photoreceptor array) followed by a cascade connection of a number of modular structures, each of which is composed of two layers of cells connected in a cascade. The first layer of each module consists of "S-cells", which show characteristics similar to simple cells or lower order hypercomplex cells, and the second layer consists of "C-cells" similar to complex cells or higher order hypercomplex cells. The afferent synapses to each S-cell have plasticity and are modifiable. The network has an ability of unsupervised learning: We do not need any "teacher" during the process of self-organization, and it is only needed to present a set of stimulus patterns repeatedly to the input layer of the network. The network has been simulated on a digital computer. After repetitive presentation of a set of stimulus patterns, each stimulus pattern has become to elicit an output only from one of the C-cell of the last layer, and conversely, this C-cell has become selectively responsive only to that stimulus pattern. That is, none of the C-cells of the last layer responds to more than one stimulus pattern. The response of the C-cells of the last layer is not affected by the pattern's position at all. Neither is it affected by a small change in shape nor in size of the stimulus pattern. {\textcopyright} 1980 Springer-Verlag.},
author = {Fukushima, Kunihiko},
doi = {10.1007/BF00344251},
issn = {03401200},
journal = {Biological Cybernetics},
month = {apr},
number = {4},
pages = {193--202},
pmid = {7370364},
publisher = {Springer-Verlag},
title = {{Neocognitron: A self-organizing neural network model for a mechanism of pattern recognition unaffected by shift in position}},
volume = {36},
year = {1980}
}
@article{LeCun1998,
abstract = {Multilayer neural networks trained with the back-propagation algorithm constitute the best example of a successful gradient-based learning technique. Given an appropriate network architecture, gradient-based learning algorithms can be used to synthesize a complex decision surface that can classify high-dimensional patterns, such as handwritten characters, with minimal preprocessing. This paper reviews various methods applied to handwritten character recognition and compares them on a standard handwritten digit recognition task. Convolutional neural networks, which are specifically designed to deal with the variability of two dimensional (2-D) shapes, are shown to outperform all other techniques. Real-life document recognition systems are composed of multiple modules including field extraction, segmentation, recognition, and language modeling. A new learning paradigm, called graph transformer networks (GTN's), allows such multimodule systems to be trained globally using gradient-based methods so as to minimize an overall performance measure. Two systems for online handwriting recognition are described. Experiments demonstrate the advantage of global training, and the flexibility of graph transformer networks. A graph transformer network for reading a bank check is also described. It uses convolutional neural network character recognizers combined with global training techniques to provide record accuracy on business and personal checks. It is deployed commercially and reads several million checks per day. {\textcopyright} 1998 IEEE.},
author = {LeCun, Yann and Bottou, L{\'{e}}on and Bengio, Yoshua and Haffner, Patrick},
doi = {10.1109/5.726791},
issn = {00189219},
journal = {Proceedings of the IEEE},
keywords = {Convolutional neural networks,Document recognition,Finite state transducers,Gradient-based learning,Graph transformer networks,Machine learning,Neural networks,Optical character recognition (OCR)},
number = {11},
pages = {2278--2323},
title = {{Gradient-based learning applied to document recognition}},
volume = {86},
year = {1998}
}
@article{Dieleman2015,
author = {Dieleman, Sander and Willett, Kyle W. and Dambre, Joni},
doi = {10.1093/mnras/stv632},
issn = {1365-2966},
journal = {Monthly Notices of the Royal Astronomical Society},
month = {jun},
number = {2},
pages = {1441--1459},
title = {{Rotation-invariant convolutional neural networks for galaxy morphology prediction}},
url = {http://academic.oup.com/mnras/article/450/2/1441/979677/Rotationinvariant-convolutional-neural-networks},
volume = {450},
year = {2015}
}
@article{Jacobs2019,
abstract = {We search Dark Energy Survey (DES) Year 3 imaging for galaxy-galaxy strong gravitational lenses using convolutional neural networks, extending previous work with new training sets and covering a wider range of redshifts and colors. We train two neural networks using images of simulated lenses, then use them to score postage stamp images of 7.9 million sources from the Dark Energy Survey chosen to have plausible lens colors based on simulations. We examine 1175 of the highest-scored candidates and identify 152 probable or definite lenses. Examining an additional 20,000 images with lower scores, we identify a further 247 probable or definite candidates. After including 86 candidates discovered in earlier searches using neural networks and 26 candidates discovered through visual inspection of blue-near-red objects in the DES catalog, we present a catalog of 511 lens candidates.},
archivePrefix = {arXiv},
arxivId = {1905.10522},
author = {Jacobs, C. and Collett, T. and Glazebrook, K. and Buckley-Geer, E. and Diehl, H. T. and Lin, H. and McCarthy, C. and Qin, A. K. and Odden, C. and Escudero, M. Caso and Dial, P. and Yung, V. J. and Gaitsch, S. and Pellico, A. and Lindgren, K. A. and Abbott, T. M. C. and Annis, J. and Avila, S. and Brooks, D. and Burke, D. L. and Rosell, A. Carnero and Kind, M. Carrasco and Carretero, J. and da Costa, L. N. and Vicente, J. De and Fosalba, P. and Frieman, J. and Garc{\'{i}}a-Bellido, J. and Gaztanaga, E. and Goldstein, D. A. and Gruen, D. and Gruendl, R. A. and Gschwend, J. and Hollowood, D. L. and Honscheid, K. and Hoyle, B. and James, D. J. and Krause, E. and Kuropatkin, N. and Lahav, O. and Lima, M. and Maia, M. A. G. and Marshall, J. L. and Miquel, R. and Plazas, A. A. and Roodman, A. and Sanchez, E. and Scarpine, V. and Serrano, S. and Sevilla-Noarbe, I. and Smith, M. and Sobreira, F. and Suchyta, E. and Swanson, M. E. C. and Tarle, G. and Vikram, V. and Walker, A. R. and Zhang, Y.},
doi = {10.3847/1538-4365/ab26b6},
eprint = {1905.10522},
file = {:home/daniel/Desktop/TOREAD/1905.10522.pdf:pdf},
issn = {0067-0049},
journal = {The Astrophysical Journal Supplement Series},
number = {1},
pages = {17},
title = {{An Extended Catalog of Galaxyâ€“Galaxy Strong Gravitational Lenses Discovered in DES Using Convolutional Neural Networks}},
volume = {243},
year = {2019}
}
@article{JMLR:v15:srivastava14a,
author = {Srivastava, Nitish and Hinton, Geoffrey and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan},
journal = {Journal of Machine Learning Research},
pages = {1929--1958},
title = {{Dropout: A Simple Way to Prevent Neural Networks from Overfitting}},
url = {http://jmlr.org/papers/v15/srivastava14a.html},
volume = {15},
year = {2014}
}
@article{Metcalf2019,
abstract = {Large-scale imaging surveys will increase the number of galaxy-scale strong lensing candidates by maybe three orders of magnitudes beyond the number known today. Finding these rare objects will require picking them out of at least tens of millions of images, and deriving scientific results from them will require quantifying the efficiency and bias of any search method. To achieve these objectives automated methods must be developed. Because gravitational lenses are rare objects, reducing false positives will be particularly important. We present a description and results of an open gravitational lens finding challenge. Participants were asked to classify 100 000 candidate objects as to whether they were gravitational lenses or not with the goal of developing better automated methods for finding lenses in large data sets. A variety of methods were used including visual inspection, arc and ring finders, support vector machines (SVM) and convolutional neural networks (CNN). We find that many of the methods will be easily fast enough to analyse the anticipated data flow. In test data, several methods are able to identify upwards of half the lenses after applying some thresholds on the lens characteristics such as lensed image brightness, size or contrast with the lens galaxy without making a single false-positive identification. This is significantly better than direct inspection by humans was able to do. Having multi-band, ground based data is found to be better for this purpose than single-band space based data with lower noise and higher resolution, suggesting that multi-colour data is crucial. Multi-band space based data will be superior to ground based data. The most difficult challenge for a lens finder is differentiating between rare, irregular and ring-like face-on galaxies and true gravitational lenses. The degree to which the efficiency and biases of lens finders can be quantified largely depends on the realism of the simulated data on which the finders are trained.},
archivePrefix = {arXiv},
arxivId = {1802.03609},
author = {Metcalf, R. B. and Meneghetti, M. and Avestruz, C. and Bellagamba, F. and Bom, C. R. and Bertin, E. and Cabanac, R. and Courbin, F. and Davies, A. and Decenci{\`{e}}re, E. and Flamary, R. and Gavazzi, R. and Geiger, M. and Hartley, P. and Huertas-Company, M. and Jackson, N. and Jacobs, C. and Jullo, E. and Kneib, J. P. and Koopmans, L. V.E. and Lanusse, F. and Li, C. L. and Ma, Q. and Makler, M. and Li, N. and Lightman, M. and Petrillo, C. E. and Serjeant, S. and Sch{\"{a}}fer, C. and Sonnenfeld, A. and Tagore, A. and Tortora, C. and Tuccillo, D. and Valent{\'{i}}n, M. B. and Velasco-Forero, S. and {Verdoes Kleijn}, G. A. and Vernardos, G.},
doi = {10.1051/0004-6361/201832797},
eprint = {1802.03609},
file = {:home/daniel/Desktop/TOREAD/1802.03609.pdf:pdf},
issn = {14320746},
journal = {Astronomy and Astrophysics},
keywords = {Gravitational lensing: strong - methods: data anal},
title = {{The strong gravitational lens finding challenge}},
volume = {625},
year = {2019}
}
